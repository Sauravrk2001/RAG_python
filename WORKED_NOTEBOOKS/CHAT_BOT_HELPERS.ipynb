{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smlst = []\n",
    "\n",
    "# def load_chunks(filename=\"preprocessed_chunks.json\"):\n",
    "#     with open(filename, 'r') as f:\n",
    "#         chunk_dicts = json.load(f)\n",
    "    \n",
    "#     for chunk in chunk_dicts:\n",
    "#         smlst.append(len(chunk[\"page_content\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_chunks()\n",
    "# smlst.sort()\n",
    "# mean_lst = sum(smlst)/len(smlst)\n",
    "# mean_lst\n",
    "\n",
    "# # 2519.6176470588234  MEAN LENGTH OF A PARAGRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is LangChain? LangChain is a framework for developing applications powered by language models. It helps in building AI apps by simplifying the process of connecting different components like models, agents, and memory. Here are some key aspects of LangChain:\n",
      "\n",
      "1. **Chaining Prompts**: LangChain allows you to create chains of prompts, where the output of one prompt is fed as input to the next. This helps in building complex, multi-step workflows.\n",
      "\n",
      "2. **Agents**: Agents in LangChain are autonomous AI entities that can perform tasks by interacting with tools and other agents. They have the capability to execute code, make API calls, and use external tools.\n",
      "\n",
      "3. **Memory**: LangChain provides various types of memory (e.g., conversation memory, file memory) to help maintain context and state across interactions.\n",
      "\n",
      "4. **Model Wrappers**: LangChain offers wrappers for various language models, making it easier to work with different models and their specific capabilities.\n",
      "\n",
      "5. **Plugins and Tools**: LangChain supports integration with external tools and plugins, allowing agents to use these tools for more complex tasks.\n",
      "\n",
      "Here's a simple example of using LangChain to create a chain of prompts:\n",
      "\n",
      "```python\n",
      "from langchain import PromptTemplate, LLMChain\n",
      "\n",
      "template = \"What is a good {subject} to watch on {medium}?\"\n",
      "\n",
      "prompt = PromptTemplate(input_variables=[\"subject\", \"medium\"], template=template)\n",
      "\n",
      "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
      "\n",
      "response = llm_chain.run(subject=\"movie\", medium=\"Netflix\")\n",
      "print(response)\n",
      "```\n",
      "\n",
      "In this example, the `LLMChain` takes a `PromptTemplate` and a language model (`llm`), and uses them to generate a response based on the given inputs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from AI_GATEWAYS import huggingface_api_key\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"meta-llama/Llama-3.3-70B-Instruct\",  # Your Llama model\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 8192},\n",
    "    huggingfacehub_api_token=huggingface_api_key\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"What is LangChain?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a joke. Are you a mushroom? Because you're a fun guy to be around.\n",
      "\n",
      "What do you call a fake noodle? An impasta.\n",
      "\n",
      "What do you call a bee that can't make up its mind? A maybe.\n",
      "\n",
      "What do you get when you cross a snowman and a vampire? Frostbite.\n",
      "\n",
      "Why can't you give Elsa a balloon? Because she will let it go.\n"
     ]
    }
   ],
   "source": [
    "# from langchain_community.llms import HuggingFaceHub\n",
    "# from AI_GATEWAYS import huggingface_api_key\n",
    "\n",
    "# llm = HuggingFaceHub(\n",
    "#     repo_id=\"tiiuae/falcon-7b-instruct\",  # Public model\n",
    "#     model_kwargs={\"temperature\": 0.5, \"max_length\": 512},\n",
    "#     huggingfacehub_api_token=huggingface_api_key\n",
    "# )\n",
    "# response = llm.invoke(\"Tell me a joke.\")\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import InferenceClient\n",
    "# from AI_GATEWAYS import huggingface_api_key\n",
    "\n",
    "# client = InferenceClient(token=huggingface_api_key)\n",
    "\n",
    "# try:\n",
    "#     model_list = client.get_model_list()\n",
    "#     print(\"API Key is valid. Models available:\", model_list)\n",
    "# except Exception as e:\n",
    "#     print(\"Invalid API Key:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key is valid! Test response: b'[{\"generated_text\":\"Hello, world!\\');\\\\n});\\\\n\\\\n// This is a test for the `.bind()`\"}]'\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import InferenceClient\n",
    "# from AI_GATEWAYS import huggingface_api_key\n",
    "\n",
    "# client = InferenceClient(token=huggingface_api_key)\n",
    "\n",
    "# try:\n",
    "#     response = client.post(model=\"bigscience/bloom\", json={\"inputs\": \"Hello, world!\"})\n",
    "#     print(\"API Key is valid! Test response:\", response)\n",
    "# except Exception as e:\n",
    "#     print(\"Invalid API Key or permission issue:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API Key is working! Response: b'[{\"generated_text\":\"Hello, world!\\');\\\\n});\\\\n\\\\n// This is a test for the `.bind()`\"}]'\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import InferenceClient\n",
    "# from AI_GATEWAYS import huggingface_api_key  # Ensure this is correct\n",
    "\n",
    "# client = InferenceClient(token=huggingface_api_key)\n",
    "\n",
    "# try:\n",
    "#     response = client.post(\n",
    "#         model=\"bigscience/bloom\",\n",
    "#         json={\"inputs\": \"Hello, world!\"}\n",
    "#     )\n",
    "#     print(\"✅ API Key is working! Response:\", response)\n",
    "# except Exception as e:\n",
    "#     print(\"❌ Invalid API Key or permission issue:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
