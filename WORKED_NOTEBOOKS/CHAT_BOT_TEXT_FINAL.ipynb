{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PARAGRAPH SPLITTER WITHOUT RECURSIVE CHARACTER TEXT SPLITTER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def split_by_paragraph(text):\n",
    "    paragraphs = re.split(r'\\n\\s*\\n+', text.strip())  # Splits on double newlines (paragraph breaks)\n",
    "    return [p.strip() for p in paragraphs if p.strip()]  # Remove empty paragraphs\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[\\n\\t\\r]+', ' ', text)\n",
    "\n",
    "# Load textbook (PDF)\n",
    "loader = PyPDFLoader(\"The-Odyssey.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Clean and split each document into paragraphs\n",
    "paragraph_chunks = []\n",
    "for doc in docs:\n",
    "    cleaned_text = clean_text(doc.page_content)\n",
    "    paragraphs = split_by_paragraph(cleaned_text)\n",
    "    for para in paragraphs:\n",
    "        paragraph_chunks.append({\"page_content\": para, \"metadata\": doc.metadata})  # Preserve metadata\n",
    "\n",
    "# Convert to LangChain Document objects\n",
    "from langchain.schema import Document\n",
    "chunks = [Document(page_content=para[\"page_content\"], metadata=para[\"metadata\"]) for para in paragraph_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SAVING THE CHUNKS AS A JSON FILE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_chunks(chunks, filename=\"preprocessed_chunks.json\"):\n",
    "    chunk_dicts = [{\"page_content\": c.page_content, \"metadata\": c.metadata} for c in chunks]\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(chunk_dicts, f)\n",
    "\n",
    "# Run this once after preprocessing\n",
    "save_chunks(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TESTING WITH VECTORIZING AFTER PARAGRAPH SPLITTER INSTEAD OF RECURSIVE CHARACTER TEXT SPLITTER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"intfloat/e5-large-v2\")\n",
    "import tqdm\n",
    "\n",
    "# Initialize Chroma vector store\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db_pedition\", embedding_function=embedding_function)\n",
    "\n",
    "# Extract text from document chunks\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "metadatas = [doc.metadata for doc in chunks]  # Optional: If your chunks have metadata\n",
    "\n",
    "# Use tqdm to show progress while adding texts to ChromaDB\n",
    "for i in tqdm(range(0, len(texts), 10), desc=\"Storing documents in ChromaDB\", unit=\"batch\"):\n",
    "    vectorstore.add_texts(texts=texts[i:i+10], metadatas=metadatas[i:i+10])  # Adding in batches\n",
    "\n",
    "# Persist the vector store to disk\n",
    "vectorstore.persist()\n",
    "\n",
    "# 9 MINUTE AND 38 SECONDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LOADING THE SAVED CHUNKS AND THE CHROMA VECTOR DATABASE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks(filename=\"preprocessed_chunks.json\"):\n",
    "    with open(filename, 'r') as f:\n",
    "        chunk_dicts = json.load(f)\n",
    "    return [Document(page_content=c[\"page_content\"], metadata=c[\"metadata\"]) for c in chunk_dicts]\n",
    "\n",
    "# Load chunks instead of reprocessing PDF\n",
    "chunks = load_chunks()\n",
    "\n",
    "# Load the stored vector database\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db_pedition\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>USING BOTH VECTOR SIMILARITY AND KEYWORD SIMILARITY</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "# vector retriever\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Keyword retriever (BM25)\n",
    "bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "# Hybrid ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    weights=[0.7, 0.3]  # tune if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HYBRID APPROACH OF USING HYBRID RETRIEVAL APPROACH FOLLOWED BY RE-RANKING</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load cross-encoder model\n",
    "# rerank_model_name = \"BAAI/bge-reranker-large\" # USE THIS IF NEEDED A PRECISE RESULT\n",
    "rerank_model_name = \"BAAI/bge-reranker-base\"  # USE THIS IF NEEDED A FASTER RESULT\n",
    "tokenizer = AutoTokenizer.from_pretrained(rerank_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(rerank_model_name)\n",
    "\n",
    "def rerank_chunks(query, chunks, top_k=3):\n",
    "    pairs = [[query, chunk.page_content] for chunk in chunks]\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = model(**inputs).logits.view(-1).float()\n",
    "\n",
    "    sorted_indices = scores.argsort(descending=True)\n",
    "    top_chunks = [chunks[i] for i in sorted_indices[:top_k]]\n",
    "    top_scores = scores[sorted_indices[:top_k]].numpy()  # Extract scores for top results\n",
    "\n",
    "    return top_chunks, top_scores \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RERANK WITH SENTENCE SPLITTER FOR  REDUCED TOKEN COUNT. BUT INTENSE COMPUTATION REQUIRED\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# from langchain.schema import Document\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# def rerank_chunks(query, chunks, top_k=3):\n",
    "#     # Split paragraphs into sentences while maintaining metadata\n",
    "#     sentences = []\n",
    "#     for chunk in chunks:\n",
    "#         try:\n",
    "#             chunk_sentences = sent_tokenize(chunk.page_content)\n",
    "#         except:\n",
    "#             # Fallback for simple sentence splitting if NLTK fails\n",
    "#             chunk_sentences = chunk.page_content.split('. ')\n",
    "        \n",
    "#         for sent in chunk_sentences:\n",
    "#             sentences.append(Document(\n",
    "#                 page_content=sent.strip(),\n",
    "#                 metadata=chunk.metadata  # Preserve original metadata\n",
    "#             ))\n",
    "\n",
    "#     # Create query-sentence pairs for scoring\n",
    "#     pairs = [[query, doc.page_content] for doc in sentences]\n",
    "#     inputs = tokenizer(pairs, padding=True, truncation=True, \n",
    "#                       return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         scores = model(**inputs).logits.view(-1).float()\n",
    "\n",
    "#     # Sort sentences by their relevance scores\n",
    "#     sorted_indices = scores.argsort(descending=True)\n",
    "#     top_sentences = [sentences[i] for i in sorted_indices[:top_k]]\n",
    "#     top_scores = scores[sorted_indices[:top_k]].numpy()\n",
    "\n",
    "#     return top_sentences, top_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_confidence(scores):\n",
    "    \"\"\"Takes the scores from `rerank_chunks` and applies sigmoid to get a confidence score.\"\"\"\n",
    "    probabilities = torch.sigmoid(torch.tensor(scores)).numpy()\n",
    "    return float(np.max(probabilities))  # Return the highest confidence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LOADING HUGGINGFACE MODELS</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from AI_GATEWAYS import huggingface_api_key\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"deepseek-ai/DeepSeek-R1\",\n",
    "    # repo_id=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    model_kwargs={\"temperature\": 0.2, \"max_length\": 1024},\n",
    "    huggingfacehub_api_token=huggingface_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LLM IMPLEMENTATION USING MEMORY AND THRESHHOLD</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Create memory that retains last 3 exchanges\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=10,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful AI assistant. Answer the user's question based on the conversation history and the provided Odyssey context. \n",
    "\n",
    "    ### Chat History:\n",
    "    {chat_history}\n",
    "\n",
    "    ### Context:\n",
    "    {context}\n",
    "\n",
    "    ### Question: \n",
    "    {question}\n",
    "\n",
    "    ### Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": lambda x: x[\"chunks\"], \n",
    "     \"question\": lambda x: x[\"question\"],\n",
    "     \"chat_history\": lambda x: x[\"chat_history\"]}\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chunks(chunks):\n",
    "    return \"\\n\\n\".join([f\"Page {c.metadata['page']}: {c.page_content}\" for c in chunks])\n",
    "\n",
    "\n",
    "# Modified ask_question function with confidence scoring\n",
    "def ask_question(question):\n",
    "    # Retrieve context\n",
    "    initial_chunks = ensemble_retriever.get_relevant_documents(question)\n",
    "    final_chunks, relevance_scores = rerank_chunks(question, initial_chunks, top_k=3)\n",
    "    \n",
    "    # Calculate confidence\n",
    "    confidence = calculate_confidence(relevance_scores)\n",
    "    \n",
    "    # Generate answer\n",
    "    raw_answer = qa_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"chunks\": format_chunks(final_chunks),\n",
    "        \"chat_history\": memory.load_memory_variables({})[\"chat_history\"]\n",
    "    })\n",
    "\n",
    "    # Extract everything after \"### Answer:\"\n",
    "    answer = raw_answer.split(\"### Answer:\")[-1].strip()\n",
    "    \n",
    "    # Store interaction in memory\n",
    "    memory.save_context({\"question\": question}, {\"answer\": answer})\n",
    "    \n",
    "    # Add confidence and sources\n",
    "    sources = list(set(c.metadata[\"page\"] for c in final_chunks))\n",
    "    response = f\"{answer}\\n\\nConfidence: {confidence:.0%}\\nSources: Pages {', '.join(map(str, sources))}\"\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DEEPSEEK RESPONSE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telemachus is the son of Odysseus and Penelope. He is the main character in Homer's Odyssey, and the story follows his journey to find out what happened to his father, who has been missing for many years.\n",
      "\n",
      "Confidence: 1%\n",
      "Sources: Pages 184, 41, 186\n"
     ]
    }
   ],
   "source": [
    "print(ask_question(\"Who is Telemachus?\"))\n",
    "\n",
    "# TOOK ONLY 10.1 SECONDS WHEREAS THE OTHER WITH SENTENCE RERANKER TOOK 37 SECONDS\n",
    "# SECOND RUN  8.9 SECONDS\n",
    "# THIRD RUN  7 SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telemachus is the son of Odysseus and Penelope. He is the main character in Homer's Odyssey, and the story follows his journey to find out what happened to his father, who has been missing for many years.\n",
      "\n",
      "Confidence: 26%\n",
      "Sources: Pages 97, 292, 230\n"
     ]
    }
   ],
   "source": [
    "print(ask_question(\"And what about his relationship with Odysseus?\"))\n",
    "\n",
    "# TOOK ONLY 17.7 SECONDS WHEREAS THE OTHER WITH SENTENCE RERANKER TOOK 42 SECONDS\n",
    "# SECOND RUN  12.4 SECONDS\n",
    "# THIRD RUN 6.3 SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, Odysseus did not use a lightsaber. Lightsabers are a fictional weapon from the Star Wars universe, while Odysseus is a character from Homer's Odyssey, set in ancient Greece. Odysseus used a bow, arrows, and a sword in the story.\n",
      "\n",
      "Confidence: 42%\n",
      "Sources: Pages 224, 268, 79\n"
     ]
    }
   ],
   "source": [
    "print(ask_question(\"Did Odysseus use a lightsaber?\"))\n",
    "\n",
    "# TOOK ONLY 10.4 SECONDS WHEREAS THE OTHER WITH SENTENCE RERANKER TOOK 57 SECONDS\n",
    "# SECOND RUN  16.6 SECONDS\n",
    "# THIRD RUN 6.5 SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telemachus is the son of Odysseus and Penelope. Odysseus is the husband of Penelope.\n",
      "\n",
      "Confidence: 3%\n",
      "Sources: Pages 279, 263, 255\n"
     ]
    }
   ],
   "source": [
    "print(ask_question(\"what is both of their relationship with penelope?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
