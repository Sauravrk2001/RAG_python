{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"intfloat/e5-large-v2\")\n",
    "from langchain.schema import Document\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LOADING THE SAVED CHUNKS AND THE CHROMA VECTOR DATABASE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks(filename=\"preprocessed_chunks.json\"):\n",
    "    with open(filename, 'r') as f:\n",
    "        chunk_dicts = json.load(f)\n",
    "    return [Document(page_content=c[\"page_content\"], metadata=c[\"metadata\"]) for c in chunk_dicts]\n",
    "\n",
    "# Load chunks instead of reprocessing PDF\n",
    "chunks = load_chunks()\n",
    "\n",
    "# Load the stored vector database\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db_pedition\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>USING BOTH VECTOR SIMILARITY AND KEYWORD SIMILARITY</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "# vector retriever\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Keyword retriever (BM25)\n",
    "bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "# Hybrid ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    weights=[0.7, 0.3]  # tune if needed\n",
    ")\n",
    "\n",
    "relevant_chunks = ensemble_retriever.get_relevant_documents(\"who wrote odyssey?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HYBRID APPROACH OF USING HYBRID RETRIEVAL APPROACH FOLLOWED BY RE-RANKING</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anandhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load cross-encoder model\n",
    "# rerank_model_name = \"BAAI/bge-reranker-large\" # USE THIS IF NEEDED A PRECISE RESULT\n",
    "rerank_model_name = \"BAAI/bge-reranker-base\"  # USE THIS IF NEEDED A FASTER RESULT\n",
    "tokenizer = AutoTokenizer.from_pretrained(rerank_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(rerank_model_name)\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from langchain.schema import Document\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def rerank_chunks(query, chunks, top_k=3):\n",
    "    # Split paragraphs into sentences while maintaining metadata\n",
    "    sentences = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            chunk_sentences = sent_tokenize(chunk.page_content)\n",
    "        except:\n",
    "            # Fallback for simple sentence splitting if NLTK fails\n",
    "            chunk_sentences = chunk.page_content.split('. ')\n",
    "        \n",
    "        for sent in chunk_sentences:\n",
    "            sentences.append(Document(\n",
    "                page_content=sent.strip(),\n",
    "                metadata=chunk.metadata  # Preserve original metadata\n",
    "            ))\n",
    "\n",
    "    # Create query-sentence pairs for scoring\n",
    "    pairs = [[query, doc.page_content] for doc in sentences]\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, \n",
    "                      return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = model(**inputs).logits.view(-1).float()\n",
    "\n",
    "    # Sort sentences by their relevance scores\n",
    "    sorted_indices = scores.argsort(descending=True)\n",
    "    top_sentences = [sentences[i] for i in sorted_indices[:top_k]]\n",
    "    top_scores = scores[sorted_indices[:top_k]].numpy()\n",
    "\n",
    "    return top_sentences, top_scores\n",
    "\n",
    "\n",
    "def calculate_confidence(scores):\n",
    "    \"\"\"Takes the scores from `rerank_chunks` and applies sigmoid to get a confidence score.\"\"\"\n",
    "    probabilities = torch.sigmoid(torch.tensor(scores)).numpy()\n",
    "    return float(np.max(probabilities))  # Return the highest confidence score\n",
    "\n",
    "\n",
    "# relevant_sentences, sentence_scores = rerank_chunks(\"who wrote odyssey?\", relevant_chunks)\n",
    "# final_confidence = calculate_confidence(sentence_scores)\n",
    "# relevant_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_chunks = ensemble_retriever.get_relevant_documents(\"who wrote odyssey?\")\n",
    "# final_chunks, relevance_scores = rerank_chunks(\"who wrote odyssey?\", initial_chunks, top_k=3)\n",
    "\n",
    "# confidence = calculate_confidence(relevance_scores)\n",
    "# final_chunks\n",
    "\n",
    "# # 16.1 SECONDS FIRST RUN\n",
    "# # 21.9 SECONDS SECOND RUN\n",
    "# # 20.5 SECONDS THIRD RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_chunks = ensemble_retriever.get_relevant_documents(\"Explain the themes of hospitality in The Odyssey.\")\n",
    "# final_chunks, relevance_scores = rerank_chunks(\"Explain the themes of hospitality in The Odyssey.\", initial_chunks, top_k=3)\n",
    "# final_chunks\n",
    "\n",
    "# # 18.0 SECONDS FIRST RUN\n",
    "# # 18.4 SECONDS SECOND RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LOADING LLAMA3 ( GROQ )</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from AI_GATEWAYS import groq_api_key\n",
    "\n",
    "llm = ChatGroq(\n",
    "    # model_name = \"deepseek-r1-distill-llama-70b\",\n",
    "    model_name = \"llama3-70b-8192\",\n",
    "    # model_name = \"mixtral-8x7b-32768\",\n",
    "    temperature=0,\n",
    "    groq_api_key = groq_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LOADING LLAMA3 ( HUGGINGFACE )</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import HuggingFaceHub\n",
    "# from AI_GATEWAYS import huggingface_api_key\n",
    "\n",
    "# llm = HuggingFaceHub(\n",
    "#     repo_id=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "#     model_kwargs={\"temperature\": 0.2, \"max_length\": 1024},\n",
    "#     huggingfacehub_api_token=huggingface_api_key\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anandhu\\AppData\\Local\\Temp\\ipykernel_16388\\3817004653.py:4: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm = HuggingFaceHub(\n"
     ]
    }
   ],
   "source": [
    "# from langchain_community.llms import HuggingFaceHub\n",
    "# from AI_GATEWAYS import huggingface_api_key\n",
    "\n",
    "# llm = HuggingFaceHub(\n",
    "#     repo_id=\"deepseek-ai/DeepSeek-R1\",\n",
    "#     model_kwargs={\"temperature\": 0.2, \"max_length\": 1024},\n",
    "#     huggingfacehub_api_token=huggingface_api_key\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import HuggingFaceHub\n",
    "# from AI_GATEWAYS import huggingface_api_key\n",
    "\n",
    "# llm = HuggingFaceHub(\n",
    "#     repo_id=\"deepseek-ai/DeepSeek-R1\",\n",
    "#     model_kwargs={\"temperature\": 0.2, \"max_length\": 1024},\n",
    "#     huggingfacehub_api_token=huggingface_api_key\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LLM IMPLEMENTATION USING MEMORY AND THRESHHOLD</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Create memory that retains last 3 exchanges\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=10,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "\n",
    "# Modified QA Chain with Memory\n",
    "qa_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer based on our conversation history and the Odyssey context:\n",
    "    \n",
    "    Chat History:\n",
    "    {chat_history}\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer in complete sentences, citing text evidence. \n",
    "    If unsure, say so:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": lambda x: x[\"chunks\"], \n",
    "     \"question\": lambda x: x[\"question\"],\n",
    "     \"chat_history\": lambda x: x[\"chat_history\"]}\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chunks(chunks):\n",
    "    return \"\\n\\n\".join([f\"Page {c.metadata['page']}: {c.page_content}\" for c in chunks])\n",
    "\n",
    "\n",
    "\n",
    "def get_threshold_response():\n",
    "    return \"I'm not entirely confident about this answer. Would you like to rephrase or ask about another topic?\"\n",
    "\n",
    "\n",
    "\n",
    "# Modified ask_question function with confidence scoring\n",
    "def ask_question(question, confidence_threshold=0.65):\n",
    "    # Retrieve context\n",
    "    initial_chunks = ensemble_retriever.get_relevant_documents(question)\n",
    "    final_chunks, relevance_scores = rerank_chunks(question, initial_chunks, top_k=3)\n",
    "    \n",
    "    # Calculate confidence\n",
    "    confidence = calculate_confidence(relevance_scores)\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = qa_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"chunks\": format_chunks(final_chunks),\n",
    "        \"chat_history\": memory.load_memory_variables({})[\"chat_history\"]\n",
    "    })\n",
    "    \n",
    "    # Store interaction in memory\n",
    "    memory.save_context({\"question\": question}, {\"answer\": answer})\n",
    "    \n",
    "    # Add confidence and sources\n",
    "    sources = list(set(c.metadata[\"page\"] for c in final_chunks))\n",
    "    response = f\"{answer}\\n\\nConfidence: {confidence:.0%}\\nSources: Pages {', '.join(map(str, sources))}\"\n",
    "    \n",
    "    return response if confidence >= confidence_threshold else f\"{get_threshold_response()}\\n\\n{response}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. IMPROVED PROMPT TEMPLATE (No chat history references)\n",
    "# qa_prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"\"\"You are an expert on Homer's Odyssey. Answer clearly and concisely using ONLY the context provided. If uncertain, state that explicitly.\n",
    "    \n",
    "#     Context:\n",
    "#     {context}\n",
    "    \n",
    "#     Current Question: {question}\"\"\"),\n",
    "#     (\"human\", \"{question}\")\n",
    "# ])\n",
    "\n",
    "# # 2. SIMPLIFIED MEMORY CONFIGURATION (Disable if not needed)\n",
    "# memory = ConversationBufferWindowMemory(\n",
    "#     k=3,  # Smaller window\n",
    "#     memory_key=\"chat_history\",\n",
    "#     return_messages=False,  # Return string instead of messages\n",
    "#     output_key=\"answer\"\n",
    "# )\n",
    "\n",
    "# # 3. CONFIDENCE CALIBRATION\n",
    "# def calculate_confidence(scores):\n",
    "#     \"\"\"Normalize scores between 0-1 using softmax\"\"\"\n",
    "#     probabilities = torch.softmax(torch.tensor(scores), dim=0).numpy()\n",
    "#     return float(np.max(probabilities))\n",
    "\n",
    "# # 4. UPDATED ASK_QUESTION FUNCTION\n",
    "# def ask_question(question, confidence_threshold=0.4):  # Lower threshold\n",
    "#     # Retrieve and rerank chunks\n",
    "#     initial_chunks = ensemble_retriever.get_relevant_documents(question)\n",
    "#     final_chunks, relevance_scores = rerank_chunks(question, initial_chunks, top_k=3)\n",
    "    \n",
    "#     # Generate answer\n",
    "#     answer = qa_chain.invoke({\n",
    "#         \"question\": question,\n",
    "#         \"context\": format_chunks(final_chunks),\n",
    "#         \"chat_history\": memory.load_memory_variables({}).get(\"chat_history\", \"\")\n",
    "#     }).strip()  # Clean whitespace\n",
    "    \n",
    "#     # Calculate calibrated confidence\n",
    "#     confidence = calculate_confidence(relevance_scores)\n",
    "    \n",
    "#     # Format response\n",
    "#     sources = list(set(c.metadata[\"page\"] for c in final_chunks))\n",
    "#     base_response = f\"{answer}\\n\\nConfidence: {confidence:.0%}\\nSources: Pages {', '.join(map(str, sources))}\"\n",
    "    \n",
    "#     return base_response if confidence >= confidence_threshold else f\"Uncertain Response: {base_response}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not entirely confident about this answer. Would you like to rephrase or ask about another topic?\n",
      "\n",
      "Human: Answer based on our conversation history and the Odyssey context:\n",
      "    \n",
      "    Chat History:\n",
      "    []\n",
      "    \n",
      "    Context:\n",
      "    Page 282: Who has set my bed otherwhere?\n",
      "\n",
      "Page 96: Who gave thee this raiment?\n",
      "\n",
      "Page 195: Howbeit, Olympian Zeus, that dwells in the clear sky, knows hereof, whether or no he will fulfill for them the evil day before their marriage.” Now even as he spake, a bird flew out on the right, a hawk, the swift messenger of Apollo.\n",
      "    \n",
      "    Question: Who is Telemachus?\n",
      "    \n",
      "    Answer in complete sentences, citing text evidence. \n",
      "    If unsure, say so:\n",
      "\n",
      "Confidence: 1%\n",
      "Sources: Pages 96, 282, 195\n"
     ]
    }
   ],
   "source": [
    "# print(ask_question(\"Who is Telemachus?\"))\n",
    "\n",
    "# # LLM DEEPSEEK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Answer based on our conversation history and the Odyssey context:\n",
      "    \n",
      "    Chat History:\n",
      "    [HumanMessage(content='Who is Telemachus?', additional_kwargs={}, response_metadata={}), AIMessage(content='Human: Answer based on our conversation history and the Odyssey context:\\n    \\n    Chat History:\\n    []\\n    \\n    Context:\\n    Page 282: Who has set my bed otherwhere?\\n\\nPage 96: Who gave thee this raiment?\\n\\nPage 195: Howbeit, Olympian Zeus, that dwells in the clear sky, knows hereof, whether or no he will fulfill for them the evil day before their marriage.” Now even as he spake, a bird flew out on the right, a hawk, the swift messenger of Apollo.\\n    \\n    Question: Who is Telemachus?\\n    \\n    Answer in complete sentences, citing text evidence. \\n    If unsure, say so:', additional_kwargs={}, response_metadata={})]\n",
      "    \n",
      "    Context:\n",
      "    Page 292: Then he communed with his heart and soul, whether he should fall on his father’s neck and kiss him, and tell him all, how he had returned and come to his own country, or whether he should first question him and prove him in every word.\n",
      "\n",
      "Page 260: Would ye stand on the side of the wooers or of Odysseus?\n",
      "\n",
      "Page 281: Meanwhile, the house-dame Eurynome had bathed the great-hearted Odysseus within his house, and anointed him with olive-oil, and cast about him a goodly mantle and a doublet.\n",
      "    \n",
      "    Question: And what about his relationship with Odysseus?\n",
      "    \n",
      "    Answer in complete sentences, citing text evidence. \n",
      "    If unsure, say so:\n",
      "\n",
      "Confidence: 98%\n",
      "Sources: Pages 281, 292, 260\n"
     ]
    }
   ],
   "source": [
    "# print(ask_question(\"And what about his relationship with Odysseus?\"))\n",
    "\n",
    "# LLM DEEPSEEK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not entirely confident about this answer. Would you like to rephrase or ask about another topic?\n",
      "\n",
      "Human: Answer based on our conversation history and the Odyssey context:\n",
      "    \n",
      "    Chat History:\n",
      "    [HumanMessage(content='Who is Telemachus?', additional_kwargs={}, response_metadata={}), AIMessage(content='Human: Answer based on our conversation history and the Odyssey context:\\n    \\n    Chat History:\\n    []\\n    \\n    Context:\\n    Page 282: Who has set my bed otherwhere?\\n\\nPage 96: Who gave thee this raiment?\\n\\nPage 195: Howbeit, Olympian Zeus, that dwells in the clear sky, knows hereof, whether or no he will fulfill for them the evil day before their marriage.” Now even as he spake, a bird flew out on the right, a hawk, the swift messenger of Apollo.\\n    \\n    Question: Who is Telemachus?\\n    \\n    Answer in complete sentences, citing text evidence. \\n    If unsure, say so:', additional_kwargs={}, response_metadata={}), HumanMessage(content='And what about his relationship with Odysseus?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Human: Answer based on our conversation history and the Odyssey context:\\n    \\n    Chat History:\\n    [HumanMessage(content='Who is Telemachus?', additional_kwargs={}, response_metadata={}), AIMessage(content='Human: Answer based on our conversation history and the Odyssey context:\\\\n    \\\\n    Chat History:\\\\n    []\\\\n    \\\\n    Context:\\\\n    Page 282: Who has set my bed otherwhere?\\\\n\\\\nPage 96: Who gave thee this raiment?\\\\n\\\\nPage 195: Howbeit, Olympian Zeus, that dwells in the clear sky, knows hereof, whether or no he will fulfill for them the evil day before their marriage.” Now even as he spake, a bird flew out on the right, a hawk, the swift messenger of Apollo.\\\\n    \\\\n    Question: Who is Telemachus?\\\\n    \\\\n    Answer in complete sentences, citing text evidence. \\\\n    If unsure, say so:', additional_kwargs={}, response_metadata={})]\\n    \\n    Context:\\n    Page 292: Then he communed with his heart and soul, whether he should fall on his father’s neck and kiss him, and tell him all, how he had returned and come to his own country, or whether he should first question him and prove him in every word.\\n\\nPage 260: Would ye stand on the side of the wooers or of Odysseus?\\n\\nPage 281: Meanwhile, the house-dame Eurynome had bathed the great-hearted Odysseus within his house, and anointed him with olive-oil, and cast about him a goodly mantle and a doublet.\\n    \\n    Question: And what about his relationship with Odysseus?\\n    \\n    Answer in complete sentences, citing text evidence. \\n    If unsure, say so:\", additional_kwargs={}, response_metadata={})]\n",
      "    \n",
      "    Context:\n",
      "    Page 268: Thence he took out four shields and eight spears, and four helmets of bronze, with thick plumes of horse hair, and he started to bring them and came quickly to his father.\n",
      "\n",
      "Page 12: Odysseus was the King of Ithaca, a small and rugged island on the western coast of Greece.\n",
      "\n",
      "Page 268: As for him he girt his fourfold shield about his shoulders and bound on his mighty head a well wrought helmet, with horse hair crest, and terribly the plume waved aloft.\n",
      "    \n",
      "    Question: Did Odysseus use a lightsaber?\n",
      "    \n",
      "    Answer in complete sentences, citing text evidence. \n",
      "    If unsure, say so:\n",
      "\n",
      "Confidence: 37%\n",
      "Sources: Pages 268, 12\n"
     ]
    }
   ],
   "source": [
    "# print(ask_question(\"Did Odysseus use a lightsaber?\"))\n",
    "\n",
    "# LLM DEEPSEEK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not entirely confident about this answer. Would you like to rephrase or ask about another topic?\n",
      "\n",
      "Human: Answer based on our conversation history and the Odyssey context:\n",
      "    \n",
      "    Chat History:\n",
      "    []\n",
      "    \n",
      "    Context:\n",
      "    Page 282: Who has set my bed otherwhere?\n",
      "\n",
      "Page 96: Who gave thee this raiment?\n",
      "\n",
      "Page 195: Howbeit, Olympian Zeus, that dwells in the clear sky, knows hereof, whether or no he will fulfill for them the evil day before their marriage.” Now even as he spake, a bird flew out on the right, a hawk, the swift messenger of Apollo.\n",
      "    \n",
      "    Question: Who is Telemachus?\n",
      "    \n",
      "    Answer in complete sentences, citing text evidence. \n",
      "    If unsure, say so:\n",
      "\n",
      "Confidence: 1%\n",
      "Sources: Pages 96, 282, 195\n"
     ]
    }
   ],
   "source": [
    "# print(ask_question(\"Who is Telemachus?\"))\n",
    "\n",
    "# LLM LLAMA 70 B HUGGINGFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Answer based on our conversation history and the Odyssey context:\n",
      "    \n",
      "    Chat History:\n",
      "    [HumanMessage(content='Who is Telemachus?', additional_kwargs={}, response_metadata={}), AIMessage(content='Human: Answer based on our conversation history and the Odyssey context:\\n    \\n    Chat History:\\n    []\\n    \\n    Context:\\n    Page 282: Who has set my bed otherwhere?\\n\\nPage 96: Who gave thee this raiment?\\n\\nPage 195: Howbeit, Olympian Zeus, that dwells in the clear sky, knows hereof, whether or no he will fulfill for them the evil day before their marriage.” Now even as he spake, a bird flew out on the right, a hawk, the swift messenger of Apollo.\\n    \\n    Question: Who is Telemachus?\\n    \\n    Answer in complete sentences, citing text evidence. \\n    If unsure, say so:', additional_kwargs={}, response_metadata={})]\n",
      "    \n",
      "    Context:\n",
      "    Page 292: Then he communed with his heart and soul, whether he should fall on his father’s neck and kiss him, and tell him all, how he had returned and come to his own country, or whether he should first question him and prove him in every word.\n",
      "\n",
      "Page 260: Would ye stand on the side of the wooers or of Odysseus?\n",
      "\n",
      "Page 281: Meanwhile, the house-dame Eurynome had bathed the great-hearted Odysseus within his house, and anointed him with olive-oil, and cast about him a goodly mantle and a doublet.\n",
      "    \n",
      "    Question: And what about his relationship with Odysseus?\n",
      "    \n",
      "    Answer in complete sentences, citing text evidence. \n",
      "    If unsure, say so:\n",
      "\n",
      "Confidence: 98%\n",
      "Sources: Pages 281, 292, 260\n"
     ]
    }
   ],
   "source": [
    "# print(ask_question(\"And what about his relationship with Odysseus?\"))\n",
    "\n",
    "# LLM LLAMA 70 B HUGGINGFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not entirely confident about this answer. Would you like to rephrase or ask about another topic?\n",
      "\n",
      "Human: Answer based on our conversation history and the Odyssey context:\n",
      "    \n",
      "    Chat History:\n",
      "    [HumanMessage(content='Who is Telemachus?', additional_kwargs={}, response_metadata={}), AIMessage(content='Human: Answer based on our conversation history and the Odyssey context:\\n    \\n    Chat History:\\n    []\\n    \\n    Context:\\n    Page 282: Who has set my bed otherwhere?\\n\\nPage 96: Who gave thee this raiment?\\n\\nPage 195: Howbeit, Olympian Zeus, that dwells in the clear sky, knows hereof, whether or no he will fulfill for them the evil day before their marriage.” Now even as he spake, a bird flew out on the right, a hawk, the swift messenger of Apollo.\\n    \\n    Question: Who is Telemachus?\\n    \\n    Answer in complete sentences, citing text evidence. \\n    If unsure, say so:', additional_kwargs={}, response_metadata={}), HumanMessage(content='And what about his relationship with Odysseus?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Human: Answer based on our conversation history and the Odyssey context:\\n    \\n    Chat History:\\n    [HumanMessage(content='Who is Telemachus?', additional_kwargs={}, response_metadata={}), AIMessage(content='Human: Answer based on our conversation history and the Odyssey context:\\\\n    \\\\n    Chat History:\\\\n    []\\\\n    \\\\n    Context:\\\\n    Page 282: Who has set my bed otherwhere?\\\\n\\\\nPage 96: Who gave thee this raiment?\\\\n\\\\nPage 195: Howbeit, Olympian Zeus, that dwells in the clear sky, knows hereof, whether or no he will fulfill for them the evil day before their marriage.” Now even as he spake, a bird flew out on the right, a hawk, the swift messenger of Apollo.\\\\n    \\\\n    Question: Who is Telemachus?\\\\n    \\\\n    Answer in complete sentences, citing text evidence. \\\\n    If unsure, say so:', additional_kwargs={}, response_metadata={})]\\n    \\n    Context:\\n    Page 292: Then he communed with his heart and soul, whether he should fall on his father’s neck and kiss him, and tell him all, how he had returned and come to his own country, or whether he should first question him and prove him in every word.\\n\\nPage 260: Would ye stand on the side of the wooers or of Odysseus?\\n\\nPage 281: Meanwhile, the house-dame Eurynome had bathed the great-hearted Odysseus within his house, and anointed him with olive-oil, and cast about him a goodly mantle and a doublet.\\n    \\n    Question: And what about his relationship with Odysseus?\\n    \\n    Answer in complete sentences, citing text evidence. \\n    If unsure, say so:\", additional_kwargs={}, response_metadata={})]\n",
      "    \n",
      "    Context:\n",
      "    Page 268: Thence he took out four shields and eight spears, and four helmets of bronze, with thick plumes of horse hair, and he started to bring them and came quickly to his father.\n",
      "\n",
      "Page 12: Odysseus was the King of Ithaca, a small and rugged island on the western coast of Greece.\n",
      "\n",
      "Page 268: As for him he girt his fourfold shield about his shoulders and bound on his mighty head a well wrought helmet, with horse hair crest, and terribly the plume waved aloft.\n",
      "    \n",
      "    Question: Did Odysseus use a lightsaber?\n",
      "    \n",
      "    Answer in complete sentences, citing text evidence. \n",
      "    If unsure, say so:\n",
      "\n",
      "Confidence: 37%\n",
      "Sources: Pages 268, 12\n"
     ]
    }
   ],
   "source": [
    "# print(ask_question(\"Did Odysseus use a lightsaber?\"))\n",
    "\n",
    "# LLM LLAMA 70 B HUGGINGFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not entirely confident about this answer. Would you like to rephrase or ask about another topic?\n",
      "\n",
      "Based on our conversation history and the Odyssey context, Telemachus is the son of Odysseus, as evidenced by the fact that he is concerned about his father's marriage and is present in the scenes where his father's belongings are being questioned, such as the bed and the raiment.\n",
      "\n",
      "Confidence: 1%\n",
      "Sources: Pages 96, 282, 195\n"
     ]
    }
   ],
   "source": [
    "print(ask_question(\"Who is Telemachus?\"))\n",
    "\n",
    "# LLM LLAMA 70B GROQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telemachus is deeply concerned about his father Odysseus' well-being and is uncertain about how to approach him, as evidenced by the fact that he communed with his heart and soul about whether to fall on his father's neck and kiss him, or to first question him and prove him in every word (Page 292). This hesitation suggests a strong emotional bond between Telemachus and Odysseus, and Telemachus' desire to reconnect with his father.\n",
      "\n",
      "Confidence: 98%\n",
      "Sources: Pages 281, 292, 260\n"
     ]
    }
   ],
   "source": [
    "print(ask_question(\"And what about his relationship with Odysseus?\"))\n",
    "\n",
    "# LLM LLAMA 70B GROQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not entirely confident about this answer. Would you like to rephrase or ask about another topic?\n",
      "\n",
      "No, Odysseus did not use a lightsaber. There is no mention of lightsabers in the provided context, and the text only describes Odysseus using traditional ancient Greek armor and weapons, such as shields, spears, and helmets with horse hair crests (Page 268).\n",
      "\n",
      "Confidence: 37%\n",
      "Sources: Pages 268, 12\n"
     ]
    }
   ],
   "source": [
    "print(ask_question(\"Did Odysseus use a lightsaber?\"))\n",
    "\n",
    "# LLM LLAMA 70B GROQ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
