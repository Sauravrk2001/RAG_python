{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"intfloat/e5-large-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nPenelope devised a challenge where the suitors had to string Odysseus' great bow and shoot an arrow through twelve axe heads. \\nNone of the suitors could accomplish the feat. However, when Odysseus, disguised as a beggar, attempted the challenge, \\nhe easily strung the bow and shot the arrow through all twelve axe heads, proving his true identity. \\n\""
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the stored vector database\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db_pedition\", embedding_function=embedding_function)\n",
    "\n",
    "# Retrieve the top 3 most similar chunks for a user query\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "relevant_chunks = retriever.get_relevant_documents(\"What challenge did Penelope use to test the suitors, and how did Odysseus prove his identity?\")\n",
    "\n",
    "\n",
    "# THE BELOW IS THE CORRECT REPLY FOR THE ABOVE ASKED ONE\n",
    "''' \n",
    "Penelope devised a challenge where the suitors had to string Odysseus' great bow and shoot an arrow through twelve axe heads. \n",
    "None of the suitors could accomplish the feat. However, when Odysseus, disguised as a beggar, attempted the challenge, \n",
    "he easily strung the bow and shot the arrow through all twelve axe heads, proving his true identity. \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>RESULTS AFTER USING PARAGRAPH SPLITTER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but he laid his hand on my mouth, and in the fulness of his wisdom suffered me not to speak. But come with me and I will stake my life on it; and if I play thee false, do thou slay me by a death most pitiful.” Then wise Penelope made answer to her: “Dear nurse, it is hard for thee, how wise soever, to observe the purposes of the everlasting gods. None the less let us go to my child, that I may see the wooers dead, and him that slew them.” With that word she went down from the upper chamber, and much her heart debated, whether she should stand apart, and question her dear lord or draw nigh, and clasp and kiss his head and hands. But when she had come within and had crossed the threshold of stone, she sat down over against Odysseus, in the light of the fire, by the further wall. Now he was sitting by the tall pillar, looking down and waiting to know if perchance his noble wife would speak to him, when her eyes beheld him. But she sat long in silence, and amazement came upon her soul, and now she would look upon him steadfastly with her eyes, and now again she knew him not, for that he was clad in vile raiment. And Telemachus rebuked her, and spake and hailed her: “Mother mine, ill mother, of an ungentle heart, why turnest thou thus away from my father, and dost not sit by him and question him and ask him all? No other woman in the world would harden her heart to stand thus aloof from her lord, who after much travail and sore had come to her in the twentieth year to his own country. But thy heart is ever harder than stone.” Then wise Penelope answered him, saying: “Child, my mind is amazed within me, and I have no strength to speak, nor to ask him aught, nay nor to look on him face to face. But if in truth this be Odysseus, and he hath indeed come home, verily we shall be ware of each other the more surely, for we have tokens that we twain know, even we, secret from all others.” So she spake, and the steadfast goodly Odysseus smiled, and quickly he spake to Telemachus winged words: “Telemachus, leave now thy mother to make trial of me within the chambers; so shall she soon come to a better knowledge than heretofore. But now I go filthy, and am clad in vile raiment, wherefore she has me in dishonour, and as yet will not allow that I am he. Let us then advise us how all may be for the very best. For whoso has slain but one man in a land, even that one leaves not many behind him to take up the feud for him, turns outlaw and leaves his kindred and his own country; but we have slain the very stay of the city, the men who were far the best of all the noble youths in Ithaca. So this I bid thee consider.” Then wise Telemachus answered him, saying: “Father, see thou to this, for\n",
      "Odysseus nodded frowning and stayed him, for all his eagerness. Then the strong prince Telemachus spake among them again: “Lo you now, even to the end of my days I shall be a coward and a weakling, or it may be I am too young, and have as yet no trust in my hands to defend me from such an one as does violence without a cause. But come now, ye who are mightier men than I, essay the bow and let us make an end of the contest.” Therewith he put the bow from him on the ground, leaning it against the smooth and well-compacted doors, and the swift shaft he propped hard by against the fair bow-tip, and then he sat down once more on the high seat, whence he had risen. Then Antinous, son of Eupeithes, spake among them, saying: “Rise up in order, all my friends, beginning from the left, even from the place whence the wine is poured.” So spake Antinous, and the saying pleased them well. Then first stood up Leiodes, son of Oenops, who was their soothsayer and ever sat by the fair mixing bowl at the extremity of the hall; he alone hated their infatuate deeds and was indignant with all the wooers. He now first took the bow and the swift shaft, and he went and stood by the threshold, and began to prove the bow; but he could not bend it; or ever that might be, his hands grew weary with the straining, his unworn, delicate hands; so he spake among the wooers, saying: “Friends, of a truth I cannot bend it, let some other take it. Ah, many of our bravest shall this bow rob of spirit and of life, since truly it is far better for us to die, than to live on and to fail of that for which we assemble evermore in this place, day by day expecting the prize. Many there be even now that hope in their hearts and desire to wed Penelope, the bedfellow of Odysseus: but when such an one shall make trial of the bow and see the issue, thereafter let him woo some other fair-robed Achaean woman with his bridal gifts and seek to win her. So may our lady wed the man that gives most gifts, and comes as the chosen of fate.” So he spake, and put from him the bow leaning it against the smooth and well- compacted doors, and the swift shaft he propped hard by against the fair bow-tip, and then he sat down once more on the high seat, whence he had risen. But Antinous rebuked him, and spake and hailed him: “Leiodes, what word hath escaped the door of thy lips; a hard word, and a grievous? Nay, it angers me to hear it, and to think that a bow such as this shall rob our bravest of spirit and of life, and all because thou canst not draw it. For I tell thee that thy lady mother bare thee not of such might as to draw a bow and shoot arrows: but there be\n",
      "“Ah, woe is me, child, for thy sake, all helpless that I am! Surely Zeus hated thee above all men, though thou hadst a god-fearing spirit! For never yet did any mortal burn so many fat pieces of the thigh and so many choice hecatombs to Zeus, whose joy is in the thunder, as thou didst give to him, praying that so thou mightest grow to a smooth old age and rear thy renowned son. But now from thee alone hath Zeus wholly cut off the day of thy returning. Haply at him too did the women mock in a strange land afar, whensoever he came to the famous palace of any lord, even as here these shameless ones all mock at thee. To shun their insults and many taunts it is that thou sufferest them not to wash thy feet, but the daughter of Icarius, wise Penelope, hath bidden me that am right willing to this task. Wherefore I will wash thy feet, both for Penelope’s sake and for thine own, for that my heart within me is moved and troubled. But come, mark the word that I shall speak. Many strangers travel-worn have ere now come hither, but I say that I have never seen any so like another, as thou art like Odysseus, in fashion in voice and in feet.” Then Odysseus of many counsels answered her saying: “Old wife, even so all men declare, that have beheld us twain, that we favour each other exceedingly, even as thou dost mark and say.” Thereupon the crone took the shining cauldron, wherefrom [34] she set to wash his feet, and poured in much cold water and next mingled therewith the warm. Now Odysseus sat aloof from the hearth, and of a sudden he turned his face to the darkness, for anon he had a misgiving of heart lest when she handled him she might know the scar again, and all should be revealed. Now she drew near her lord to wash him, and straightway she knew the scar of the wound, that the boar had dealt him with his white tusk long ago, when Odysseus went to Parnassus to see Autolycus, and the sons of Autolycus, his mother’s noble father, who outdid all men in thievery and skill in swearing. This skill was the gift of the god himself, even Hermes, for that he burned to him the well-pleasing sacrifice of the thighs of lambs and kids; wherefore Hermes abetted him gladly. Now Autolycus once had gone to the rich land of Ithaca, and found his daughter’s son a child new-born, and when he was making an end of supper, behold, Eurycleia set the babe on his knees, and spake and hailed him: “Autolycus find now a name thyself to give thy child’s own son; for lo, he is a child of many prayers.” [34] Reading τοῦ. Then Autolycus made answer and spake: “My daughter and my daughter’s lord, give ye him whatsoever name I tell you. Forasmuch as I am come hither in\n"
     ]
    }
   ],
   "source": [
    "for chunk in relevant_chunks:\n",
    "    print(chunk.page_content)  # Shows the most relevant textbook sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"intfloat/e5-large-v2\")\n",
    "from langchain.schema import Document\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LOADING THE SAVED CHUNKS AND THE CHROMA VECTOR DATABASE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks(filename=\"preprocessed_chunks.json\"):\n",
    "    with open(filename, 'r') as f:\n",
    "        chunk_dicts = json.load(f)\n",
    "    return [Document(page_content=c[\"page_content\"], metadata=c[\"metadata\"]) for c in chunk_dicts]\n",
    "\n",
    "# Load chunks instead of reprocessing PDF\n",
    "chunks = load_chunks()\n",
    "\n",
    "# Load the stored vector database\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db_pedition\", embedding_function=embedding_function)\n",
    "\n",
    "# Retrieve the top 3 most similar chunks for a user query\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>USING BOTH VECTOR SIMILARITY AND KEYWORD SIMILARITY</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 21, 'page_label': '22', 'source': 'The-Odyssey.pdf'}, page_content='The Odyssey'),\n",
       " Document(metadata={'page': 1, 'page_label': '2', 'source': 'The-Odyssey.pdf'}, page_content=\"The Project Gutenberg EBook of The Odyssey, by Homer This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you'll have to check the laws of the country where you are located before using this ebook. Title: The Odyssey Author: Homer Translator: Butcher & Lang Release Date: April, 1999 [EBook #1728] Last updated: April 16, 2020 Language: English *** START OF THIS PROJECT GUTENBERG EBOOK THE ODYSSEY *** Produced by Jim Tinsley\"),\n",
       " Document(metadata={'page': 3, 'page_label': '4', 'source': 'The-Odyssey.pdf'}, page_content='by Homer DONE INTO ENGLISH PROSE by S. H. BUTCHER, M.A. Fellow and Protector of University College, Oxford Late Fellow of Trinity College, Cambridge AND A. LANG, M.A. Late Fellow of Merton College, Oxford Contents PREFACE. PREFACE TO THE THIRD EDITION. INTRODUCTION. The Odyssey BOOK I. BOOK II.'),\n",
       " Document(metadata={'page': 20, 'page_label': '21', 'source': 'The-Odyssey.pdf'}, page_content='separated and combined. The germ of the whole epic is probably the popular tale, known all over the world, of the warrior who, on his return from a long expedition, has great difficulty in making his prudent wife recognise him. The incident occurs as a detached story in China, and in most European countries it is told of a crusader. “We may suppose it to be older than the legend of Troy, and to have gravitated into the cycle of that legend. The years of the hero’s absence are then filled up with adventures (the Cyclops, Circé, the Phaeacians, the Sirens, the descent into hell) which exist as scattered tales, or are woven into the more elaborate epics of Gaels, Aztecs, Hindoos, Tartars, South-Sea Islanders, Finns, Russians, Scandinavians, and Eskimo. The whole is surrounded with the atmosphere of the kingly age of Greece, and the result is the Odyssey, with that unity of plot and variety of character which must have been given by one masterly constructive genius. The date at which the poet of the Odyssey lived may be approximately determined by his consistent descriptions of a peculiar and definite condition of society, which had ceased to exist in the ninth century B.C., and of a stage of art in which Phoenician and Assyrian influences predominated. ( Die Kunst bei Homer. Brunn.) As to the mode of composition, it would not be difficult to show that at least the a priori Wolfian arguments against the early use of writing for literary purposes have no longer the cogency which they were once thought to possess. But this is matter for a separate investigation.'),\n",
       " Document(metadata={'source': 'The-Odyssey.pdf', 'page': 144, 'page_label': '145'}, page_content='she was queen of Pylos, and bare glorious children to her lord, Nestor and Chromius, and princely Periclymenus, and stately Pero too, the wonder of all men. All that dwelt around were her wooers; but Neleus would not give her, save to him who should drive off from Phylace the kine of mighty Iphicles, with shambling gait and broad of brow, hard cattle to drive. And none but the noble seer [19] took in hand to drive them; but a grievous fate from the gods fettered him, even hard bonds and the herdsmen of the wild. But when at length the months and days were being fulfilled, as the year returned upon his course, and the seasons came round, then did mighty Iphicles set him free, when he had spoken out all the oracles; and herein was the counsel of Zeus being accomplished. [19] Melampus “And I saw Lede, the famous bed-fellow of Tyndareus, who bare to Tyndareus two sons, hardy of heart, Castor tamer of steeds, and Polydeuces the boxer. These twain yet live, but the quickening earth is over them; and even in the nether world they have honour at the hand of Zeus. And they possess their life in turn, living one day and dying the next, and they have gotten worship even as the gods. “And after her I beheld Iphimedeia, bed-fellow of Aloeus, who said that she had lain with Poseidon, and she bare children twain, but short of life were they, godlike Otus and far-famed Ephialtes. Now these were the tallest men that earth, the graingiver, ever reared, and far the goodliest after the renowned Orion. At nine seasons old they were of breadth nine cubits, and nine fathoms in height. They it was who threatened to raise even against the immortals in Olympus the din of stormy war. They strove to pile Ossa on Olympus, and on Ossa Pelion with the trembling forest leaves, that there might be a pathway to the sky. Yea, and they would have accomplished it, had they reached the full measure of manhood. But the son of Zeus, whom Leto of the fair locks bare, destroyed the twain, ere the down had bloomed beneath their temples, and darkened their chins with the blossom of youth. “And Phaedra and Procris I saw, and fair Ariadne, the daughter of wizard Minos, whom Theseus on a time was bearing from Crete to the hill of sacred Athens, yet had he no joy of her; for Artemis slew her ere that in sea-girt Dia, by reason of the witness of Dionysus. “And Maera and Clymene I saw, and hateful Eriphyle, who took fine gold for the price of her dear lord’s life. But I cannot tell or name all the wives and daughters of the heroes that I saw; ere that, the immortal night would wane. Nay,'),\n",
       " Document(metadata={'source': 'The-Odyssey.pdf', 'page': 143, 'page_label': '144'}, page_content='the child of noble Salmoneus, and declared herself the wife of Cretheus, son of Aeolus. She loved a river, the divine Enipeus, far the fairest of the floods that run upon the earth, and she would resort to the fair streams of Enipeus. And it came to pass that the girdler of the world, the Earth-shaker, put on the shape of the god, and lay by the lady at the mouths of the whirling stream. Then the dark wave stood around them like a hill-side bowed, and hid the god and the mortal woman. And he undid her maiden girdle, and shed a slumber over her. Now when the god had done the work of love, he clasped her hand and spake and hailed her: “‘Woman, be glad in our love, and when the year comes round thou shalt give birth to glorious children,—for not weak are the embraces of the gods,—and do thou keep and cherish them. And now go home and hold thy peace, and tell it not: but behold, I am Poseidon, shaker of the earth.’ “Therewith he plunged beneath the heaving deep. And she conceived and bare Pelias and Neleus, who both grew to be mighty men, servants of Zeus. Pelias dwelt in wide Iolcos, and was rich in flocks; and that other abode in sandy Pylos. And the queen of women bare yet other sons to Cretheus, even Aeson and Pheres and Amythaon, whose joy was in chariots. “And after her I saw Antiope, daughter of Asopus, and her boast was that she had slept even in the arms of Zeus, and she bare two sons, Amphion and Zethus, who founded first the place of seven-gated Thebes, and they made of it a fenced city, for they might not dwell in spacious Thebes unfenced, for all their valiancy. “Next to her I saw Alcmene, wife of Amphitryon, who lay in the arms of mighty Zeus, and bare Heracles of the lion-heart, steadfast in the fight. And I saw Megara, daughter of Creon, haughty of heart, whom the strong and tireless son of Amphitryon had to wife. “And I saw the mother of Oedipodes, fair Epicaste, who wrought a dread deed unwittingly, being wedded to her own son, and he that had slain his own father wedded her, and straightway the gods made these things known to men. Yet he abode in pain in pleasant Thebes, ruling the Cadmaeans, by reason of the deadly counsels of the gods. But she went down to the house of Hades, the mighty warder; yea, she tied a noose from the high beam aloft, being fast holden in sorrow; while for him she left pains behind full many, even all that the Avengers of a mother bring to pass. “And I saw lovely Chloris, whom Neleus wedded on a time for her beauty, and brought gifts of wooing past number. She was the youngest daughter of Amphion, son of Iasus, who once ruled mightily in Minyan Orchomenus. And'),\n",
       " Document(metadata={'source': 'The-Odyssey.pdf', 'page': 18, 'page_label': '19'}, page_content='an old man, and bids him go to the hut of the swineherd Eumaeus, who is loyal to his absent lord. Athene then goes to Lacedaemon, to bring back Telemachus, who has now resided there for a month. Odysseus won the heart of Eumaeus, who of course did not recognise him, and slept in the swineherd’s hut, while Athene was waking Telemachus, in Lacedaemon, and bidding him “be mindful of his return.” DAY 37 (Book xv). Is spent by Odysseus in the swineherd’s hut. Telemachus reaches Pherae, half- way to Pylos. DAY 38 (Book xv). Telemachus reaches Pylos, but does not visit Nestor. To save time he goes at once on board ship, taking with him an unfortunate outlaw, Theoclymenus, a second-sighted man, or the family of Melampus, in which the gift of prophecy was hereditary. The ship passed the Elian coast at night, and evaded the ambush of the wooers. Meanwhile Odysseus was sitting up almost till dawn, listening to the history of Eumaeus, the swineherd. DAY 39 (Books xv, xvi). Telemachus reaches the Isle of Ithaca, sends his ship to the city, but himself, by advice of Athene, makes for the hut of Eumaeus, where he meets, but naturally does not recognise, his disguised father. He sends Eumaeus to Penelope with news of his arrival, and then Athene reveals Odysseus to Telemachus. The two plot the death of the wooers. Odysseus bids Telemachus remove, on a favourable opportunity, the arms which were disposed as trophies on the walls of the hall at home. (There is a slight discrepancy between the words of this advice and the manner in which it is afterwards executed.) During this interview, the ship of Telemachus, the wooers who had been in ambush, and Eumaeus, all reached the town of Ithaca. In the evening Eumaeus returned to his hut, where Athene had again disguised Odysseus. DAY 40 (Books xvii, xviii, xix, xx). The story is now hastening to its close, and many events are crowded into the fortieth day. Telemachus goes from the swineherd’s hut to the city, and calls his guest, Theoclymenus, to the palace. The second-sighted man prophesies of the near revenge of Odysseus. In the afternoon, Odysseus (still disguised) and'),\n",
       " Document(metadata={'source': 'The-Odyssey.pdf', 'page': 23, 'page_label': '24'}, page_content='the Immortals: “Lo you now, how vainly mortal men do blame the gods! For of us they say comes evil, whereas they even of themselves, through the blindness of their own hearts, have sorrows beyond that which is ordained. Even as of late Aegisthus, beyond that which was ordained, took to him the wedded wife of the son of Atreus, and killed her lord on his return, and that with sheer doom before his eyes, since we had warned him by the embassy of Hermes the keen-sighted, the slayer of Argos, that he should neither kill the man, nor woo his wife. For the son of Atreus shall be avenged at the hand of Orestes, so soon as he shall come to man’s estate and long for his own country. So spake Hermes, yet he prevailed not on the heart of Aegisthus, for all his good will; but now hath he paid one price for all.” And the goddess, grey-eyed Athene, answered him, saying: “O father, our father Cronides, throned in the highest; that man assuredly lies in a death that is his due; so perish likewise all who work such deeds! But my heart is rent for wise Odysseus, that hapless one, who far from his friends this long while suffereth affliction in a seagirt isle, where is the navel of the sea, a woodland isle, and therein a goddess hath her habitation, the daughter of the wizard Atlas, who knows the depths of every sea, and himself upholds the tall pillars which keep earth and sky asunder. His daughter it is that holds the hapless man in sorrow: and ever with soft and guileful tales she is wooing him to forgetfulness of Ithaca. But Odysseus yearning to see if it were but the smoke leap upwards from his own land, hath a desire to die. As for thee, thine heart regardeth it not at all, Olympian! What! did not Odysseus by the ships of the Argives make thee free offering of sacrifice in the wide Trojan land? Wherefore wast thou then so wroth with him, O Zeus?” And Zeus the cloud-gatherer answered her, and said, “My child, what word hath escaped the door of thy lips? Yea, how should I forget divine Odysseus, who in understanding is beyond mortals and beyond all men hath done sacrifice to the deathless gods, who keep the wide heaven? Nay, but it is Poseidon, the girdler of the earth, that hath been wroth continually with quenchless anger for the Cyclops’ sake whom he blinded of his eye, even godlike Polyphemus whose power is mightiest amongst all the Cyclôpes. His mother was the nymph Thoösa, daughter of Phorcys, lord of the unharvested sea, and in the hollow caves she lay with Poseidon. From that day forth Poseidon the earth-shaker doth not indeed slay Odysseus, but driveth him wandering from his own country. But come, let us here one and all take good counsel as touching his returning, that he may be got home; so shall Poseidon let go his displeasure, for he will in no wise be able'),\n",
       " Document(metadata={'source': 'The-Odyssey.pdf', 'page': 185, 'page_label': '186'}, page_content='immortals who hath thee in his keeping and protection will send thee a fair breeze in thy wake. But when thou hast touched the nearest shore of Ithaca, send thy ship and all thy company forward to the city, but for thy part seek first the swineherd who keeps thy swine, loyal and at one with thee. There do thou rest the night, and bid him go to the city to bear tidings of thy coming to the wise Penelope, how that she hath got thee safe, and thou art come up out of Pylos.” Therewith she departed to high Olympus. But Telemachus woke the son of Nestor out of sweet sleep, touching him with his heel, and spake to him, saying: “Awake, Peisistratus, son of Nestor, bring up thy horses of solid hoof, and yoke them beneath the car, that we may get forward on the road.” Then Peisistratus, son of Nestor, answered him, saying: “Telemachus, we may in no wise drive through the dark night, how eager soever to be gone; nay, soon it will be dawn. Tarry then, till the hero, the son of Atreus, spear-famed Menelaus, brings gifts, and sets them on the car, and bespeaks thee kindly, and sends thee on thy way. For of him a guest is mindful all the days of his life, even of the host that shows him loving-kindness.” So spake he, and anon came the golden-throned Dawn. And Menelaus, of the loud war cry, drew nigh to them, new risen from his bed, by fair-haired Helen. Now when the dear son of Odysseus marked him, he made haste and girt his shining doublet about him, and the hero cast a great mantle over his mighty shoulders, and went forth at the door, and Telemachus, dear son of divine Odysseus, came up and spake to Menelaus, saying: “Menelaus, son of Atreus, fosterling of Zeus, leader of the people, even now do thou speed me hence, to mine own dear country; for even now my heart is fain to come home again.” Then Menelaus, of the loud war cry, answered him: “Telemachus, as for me, I will not hold thee a long time here, that art eager to return; nay, I think it shame even in another host, who loves overmuch or hates overmuch. Measure is best in all things. He does equal wrong who speeds a guest that would fain abide, and stays one who is in haste to be gone. Men should lovingly entreat the present guest and speed the parting. But abide till I bring fair gifts and set them on the car and thine own eyes behold them, and I bid the women to prepare the midday meal in the halls, out of the good store they have within. Honour and glory it is for us, and gain withal for thee, that ye should have eaten well ere ye go on your way, over vast and limitless lands. What and if thou art minded to pass through Hellas and mid Argos? So shall I too go with thee, and yoke thee horses and lead thee to the towns of men, and none shall send us empty away, but will give us')]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "# Existing vector retriever\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Keyword retriever (BM25)\n",
    "bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "# Hybrid ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    weights=[0.7, 0.3]  # Tune based on data\n",
    ")\n",
    "\n",
    "relevant_chunks = ensemble_retriever.get_relevant_documents(\"who wrote odyssey?\")\n",
    "\n",
    "relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Load cross-encoder model\n",
    "rerank_model_name = \"BAAI/bge-reranker-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(rerank_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(rerank_model_name)\n",
    "\n",
    "\n",
    "\n",
    "def rerank_chunks(query, chunks, top_k=3, max_context_tokens=600):\n",
    "    \"\"\"Token-aware reranking\"\"\"\n",
    "    pairs = [[query, chunk.page_content] for chunk in chunks]\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = model(**inputs).logits.view(-1).float()\n",
    "    \n",
    "    sorted_indices = scores.argsort(descending=True)\n",
    "    selected_chunks = []\n",
    "    token_count = 0\n",
    "    \n",
    "    for idx in sorted_indices:\n",
    "        chunk = chunks[idx]\n",
    "        chunk_content = f\"Page {chunk.metadata['page']}: {chunk.page_content}\"\n",
    "        chunk_tokens = tokenizer.encode(chunk_content, add_special_tokens=False)\n",
    "        \n",
    "        if token_count + len(chunk_tokens) > max_context_tokens:\n",
    "            continue  # Skip if over limit\n",
    "            \n",
    "        selected_chunks.append(chunk)\n",
    "        token_count += len(chunk_tokens)\n",
    "        \n",
    "        if len(selected_chunks) >= top_k:\n",
    "            break\n",
    "            \n",
    "    return selected_chunks, scores[sorted_indices[:len(selected_chunks)]]\n",
    "\n",
    "\n",
    "\n",
    "def calculate_confidence(scores):\n",
    "    if len(scores) == 0:\n",
    "        return 0.0  # default confidence score if no relevant chunks were found\n",
    "    \n",
    "    probabilities = torch.sigmoid(torch.tensor(scores)).numpy()\n",
    "    return float(np.max(probabilities))  # Return the highest confidence score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anandhu\\AppData\\Local\\Temp\\ipykernel_5520\\1170359079.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  probabilities = torch.sigmoid(torch.tensor(scores)).numpy()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 1, 'page_label': '2', 'source': 'The-Odyssey.pdf'}, page_content=\"The Project Gutenberg EBook of The Odyssey, by Homer This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you'll have to check the laws of the country where you are located before using this ebook. Title: The Odyssey Author: Homer Translator: Butcher & Lang Release Date: April, 1999 [EBook #1728] Last updated: April 16, 2020 Language: English *** START OF THIS PROJECT GUTENBERG EBOOK THE ODYSSEY *** Produced by Jim Tinsley\"),\n",
       " Document(metadata={'page': 21, 'page_label': '22', 'source': 'The-Odyssey.pdf'}, page_content='The Odyssey'),\n",
       " Document(metadata={'page': 3, 'page_label': '4', 'source': 'The-Odyssey.pdf'}, page_content='by Homer DONE INTO ENGLISH PROSE by S. H. BUTCHER, M.A. Fellow and Protector of University College, Oxford Late Fellow of Trinity College, Cambridge AND A. LANG, M.A. Late Fellow of Merton College, Oxford Contents PREFACE. PREFACE TO THE THIRD EDITION. INTRODUCTION. The Odyssey BOOK I. BOOK II.')]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_chunks = ensemble_retriever.get_relevant_documents(\"who wrote odyssey?\")\n",
    "final_chunks, relevance_scores = rerank_chunks(\"who wrote odyssey?\", initial_chunks, top_k=3)\n",
    "\n",
    "confidence = calculate_confidence(relevance_scores)\n",
    "final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (754 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 21, 'page_label': '22', 'source': 'The-Odyssey.pdf'}, page_content='The Odyssey'),\n",
       " Document(metadata={'source': 'The-Odyssey.pdf', 'page': 5, 'page_label': '6'}, page_content='As one that for a weary space has lain Lulled by the song of Circe and her wine In gardens near the pale of Proserpine, Where that Ææan isle forgets the main, And only the low lutes of love complain, And only shadows of wan lovers pine, As such an one were glad to know the brine Salt on his lips, and the large air again, So gladly, from the songs of modern speech Men turn, and see the stars, and feel the free Shrill wind beyond the close of heavy flowers, And through the music of the languid hours They hear like Ocean on a western beach The surge and thunder of the Odyssey. A. L.'),\n",
       " Document(metadata={'source': 'The-Odyssey.pdf', 'page': 9, 'page_label': '10'}, page_content='from the received view, and followed Mr. Raper, who, however, has not been able to read through the proof-sheets further than Book xii. We have adopted La Roche’s text (Homeri Odyssea, J. La Roche, Leipzig, 1867), except in a few cases where we mention our reading in a foot-note. The Arguments prefixed to the Books are taken, with very slight alterations, from Hobbes’ Translation of the Odyssey. It is hoped that the Introduction added to the second edition may illustrate the growth of those national legends on which Homer worked, and may elucidate the plot of the Odyssey.')]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_chunks = ensemble_retriever.get_relevant_documents(\"Explain the themes of hospitality in The Odyssey.\")\n",
    "final_chunks, relevance_scores = rerank_chunks(\"Explain the themes of hospitality in The Odyssey.\", initial_chunks, top_k=3)\n",
    "final_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LOADING LLAMA3</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from AI_GATEWAYS import groq_api_key\n",
    "\n",
    "llm = ChatGroq(\n",
    "    # model_name = \"deepseek-r1-distill-llama-70b\",\n",
    "    model_name = \"llama3-70b-8192\",\n",
    "    # model_name = \"mixtral-8x7b-32768\",\n",
    "    temperature=0,\n",
    "    groq_api_key = groq_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LLM IMPLEMENTATION USING MEMORY AND THRESHHOLD</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Create memory that retains last 3 exchanges\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=8,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "\n",
    "# Modified QA Chain with Memory\n",
    "qa_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer based on our conversation history or the Odyssey context. Not from external knowledge:\n",
    "    \n",
    "    Chat History:\n",
    "    {chat_history}\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer in complete sentences, citing text evidence. \n",
    "    If unsure, say so:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": lambda x: x[\"chunks\"], \n",
    "     \"question\": lambda x: x[\"question\"],\n",
    "     \"chat_history\": lambda x: x[\"chat_history\"]}\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chunks(chunks):\n",
    "    return \"\\n\\n\".join([f\"Page {c.metadata['page']}: {c.page_content}\" for c in chunks])\n",
    "\n",
    "\n",
    "def get_threshold_response():\n",
    "    \"\"\"Response when confidence is below threshold\"\"\"\n",
    "    return \"I'm not entirely confident about this answer. Would you like to rephrase or ask about another topic?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anandhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Load lightweight embedding model for fast sentence filtering\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def extract_relevant_sentences(query, chunks, top_k=3):\n",
    "    \"\"\"Splits paragraphs into sentences and retrieves the most relevant ones using embeddings & reranking.\"\"\"\n",
    "    question_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    sentence_scores = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        sentences = sent_tokenize(chunk.page_content)  # Split paragraph into sentences\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_embedding = embedding_model.encode(sentence, convert_to_tensor=True)\n",
    "            score = util.pytorch_cos_sim(question_embedding, sentence_embedding).item()  # Compute similarity\n",
    "            sentence_scores.append((sentence, score))\n",
    "\n",
    "    # Sort sentences by similarity score\n",
    "    sorted_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Keep only top-N sentences before reranking\n",
    "    filtered_sentences = [sent for sent, score in sorted_sentences[:10]]  # Keep more before reranking\n",
    "\n",
    "    # Check if there are any sentences to rerank\n",
    "    if not filtered_sentences:\n",
    "        return \"No relevant sentences found.\"  # Early return if no sentences\n",
    "\n",
    "    # Create rerank pairs\n",
    "    rerank_pairs = [[query, sentence] for sentence in filtered_sentences]\n",
    "\n",
    "    # Debugging: Print rerank_pairs to see if it contains invalid entries\n",
    "    print(f\"Rerank Pairs: {rerank_pairs}\")  # Debugging: Ensure correct format\n",
    "\n",
    "    try:\n",
    "        inputs = tokenizer(rerank_pairs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during tokenization: {e}\")\n",
    "        return \"Error during tokenization.\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = model(**inputs).logits.view(-1).float()\n",
    "\n",
    "    sorted_indices = scores.argsort(descending=True)\n",
    "    top_sentences = [filtered_sentences[i] for i in sorted_indices[:top_k]]\n",
    "\n",
    "    return \"\\n\\n\".join(top_sentences) if top_sentences else \"No relevant sentences found.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, confidence_threshold=0.65):\n",
    "    # Retrieve context (Top-K Chunks)\n",
    "    initial_chunks = ensemble_retriever.get_relevant_documents(question)\n",
    "    final_chunks, relevance_scores = rerank_chunks(question, initial_chunks, top_k=3)\n",
    "\n",
    "    # Calculate confidence\n",
    "    confidence = calculate_confidence(relevance_scores)\n",
    "\n",
    "    # Extract only the most relevant sentences\n",
    "    optimized_context = extract_relevant_sentences(question, final_chunks, top_k=3)\n",
    "\n",
    "    # Generate answer\n",
    "    answer = qa_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"chunks\": optimized_context,  # Sending only relevant sentences\n",
    "        \"chat_history\": memory.load_memory_variables({})[\"chat_history\"]\n",
    "    })\n",
    "\n",
    "    # Store interaction in memory\n",
    "    memory.save_context({\"question\": question}, {\"answer\": answer})\n",
    "\n",
    "    # Add confidence and sources\n",
    "    sources = list(set(c.metadata[\"page\"] for c in final_chunks))\n",
    "    response = f\"{answer}\\n\\nConfidence: {confidence:.0%}\\nSources: Pages {', '.join(map(str, sources))}\"\n",
    "\n",
    "    return response if confidence >= confidence_threshold else f\"{get_threshold_response()}\\n\\n{response}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anandhu\\AppData\\Local\\Temp\\ipykernel_5520\\1170359079.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  probabilities = torch.sigmoid(torch.tensor(scores)).numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerank Pairs: [['Who is Telemachus?', 'Telemachus landed, goes first to Eumaeus.'], ['Who is Telemachus?', 'And she found Telemachus, and the glorious son of Nestor, couched at the vestibule of the house of famous Menelaus.'], ['Who is Telemachus?', 'Pallas sends home Telemachus from Lacedaemon with the presents given him by Menelaus.'], ['Who is Telemachus?', 'The son of Nestor truly was overcome with soft sleep, but sweet sleep gat not hold of Telemachus, but, through the night divine, careful thoughts for his father kept him wakeful.'], ['Who is Telemachus?', 'And grey-eyed Athene stood nigh him and spake to him, saying: “Telemachus, it is no longer meet that thou shouldest wander far from thy home, leaving thy substance behind thee, and men in thy house so wanton, lest they divide and utterly devour all thy wealth, and thou shalt have gone on a vain journey.'], ['Who is Telemachus?', 'Now Pallas Athene went to the wide land of Lacedaemon, to put the noble son of the great-hearted Odysseus in mind of his return, and to make him hasten his coming.'], ['Who is Telemachus?', 'Nay, keep thy well- wrought ship far from those isles, and sail by night as well as day, and he of the'], ['Who is Telemachus?', 'For even now her father and her brethren bid her wed Eurymachus, for he outdoes all the wooers in his presents, and hath been greatly increasing his gifts of wooing.'], ['Who is Telemachus?', 'The noblest of the wooers lie in wait for thee of purpose, in the strait between Ithaca and rugged Samos, eager to slay thee before thou come to thine own country.'], ['Who is Telemachus?', 'But come, rouse with all haste Menelaus, of the loud war-cry, to send thee on thy way, that thou mayest even yet find thy noble mother in her home.']]\n",
      "I'm not entirely confident about this answer. Would you like to rephrase or ask about another topic?\n",
      "\n",
      "Telemachus is the noble son of the great-hearted Odysseus, as stated in the context: \"to put the noble son of the great-hearted Odysseus in mind of his return, and to make him hasten his coming.\"\n",
      "\n",
      "Confidence: 1%\n",
      "Sources: Pages 184\n"
     ]
    }
   ],
   "source": [
    "print(ask_question(\"Who is Telemachus?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not entirely confident about this answer. Would you like to rephrase or ask about another topic?\n",
      "\n",
      "According to the provided chat history, Telemachus is the noble son of the great-hearted Odysseus.\n",
      "\n",
      "Confidence: 0%\n",
      "Sources: Pages \n"
     ]
    }
   ],
   "source": [
    "print(ask_question(\"And what about his relationship with Odysseus?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not entirely confident about this answer. Would you like to rephrase or ask about another topic?\n",
      "\n",
      "There is no mention of Odysseus using a lightsaber in our conversation history or the provided context. In fact, the concept of a lightsaber is not even mentioned at all.\n",
      "\n",
      "Confidence: 0%\n",
      "Sources: Pages \n"
     ]
    }
   ],
   "source": [
    "print(ask_question(\"Did Odysseus use a lightsaber?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anandhu\\AppData\\Local\\Temp\\ipykernel_5520\\1170359079.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  probabilities = torch.sigmoid(torch.tensor(scores)).numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerank Pairs: [['What challenge did Penelope use to test the suitors?', 'And as when a man hath hidden away a brand in the black embers at an upland farm, one that hath no neighbours nigh, and so saveth the seed of fire, that he may not have to seek a light otherwhere, even so did Odysseus cover him with the leaves.'], ['What challenge did Penelope use to test the suitors?', 'Through these the force of the wet winds blew never, neither did the bright sun light on it with his rays, nor could the rain pierce through, so close were they twined either to other; and thereunder crept Odysseus and anon he heaped together with his hands a broad couch; for of fallen leaves there was great plenty, enough to cover two or three men in winter time, however hard the weather.'], ['What challenge did Penelope use to test the suitors?', 'And the steadfast goodly Odysseus beheld it and rejoiced, and he laid him in the midst thereof and flung over him the fallen leaves.'], ['What challenge did Penelope use to test the suitors?', 'Then Odysseus turned from the river, and fell back in the reeds, and kissed earth, the grain-giver, and heavily he spake unto his own brave spirit: “Ah, woe is me!'], ['What challenge did Penelope use to test the suitors?', 'And the great wave bare it back down the stream, and lightly Ino caught it in her hands.'], ['What challenge did Penelope use to test the suitors?', 'What is to betide me?'], ['What challenge did Penelope use to test the suitors?', 'So he crept beneath twin bushes that grew from one stem, both olive trees, one of them wild olive.'], ['What challenge did Penelope use to test the suitors?', 'If I watch the river bed all through the careful night, I fear that the bitter frost and fresh dew may overcome me, as I breathe forth my life for faintness, for the river breeze blows cold betimes in the morning.'], ['What challenge did Penelope use to test the suitors?', 'But when now his breath returned and his spirit came to him again, he loosed from off him the veil of the goddess, and let it fall into the salt flowing river.'], ['What challenge did Penelope use to test the suitors?', 'He went up to the wood, and found it nigh the water in a place of wide prospect.']]\n",
      "I'm not entirely confident about this answer. Would you like to rephrase or ask about another topic?\n",
      "\n",
      "I'm unsure about the answer to this question because there is no mention of Penelope or the suitors in our conversation history or the provided context.\n",
      "\n",
      "Confidence: 2%\n",
      "Sources: Pages 82\n"
     ]
    }
   ],
   "source": [
    "print(ask_question(\"What challenge did Penelope use to test the suitors?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anandhu\\AppData\\Local\\Temp\\ipykernel_5520\\1170359079.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  probabilities = torch.sigmoid(torch.tensor(scores)).numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerank Pairs: [['How did Odysseus prove his identity to Penelope?', 'Odysseus  maketh  himself  known  to  Penelope,  tells  his adventures briefly, and in the morning goes to Laertes and makes himself known to him.'], ['How did Odysseus prove his identity to Penelope?', 'Odysseus hath come, and hath got him to his own house, though late hath he come, and hath slain the proud wooers that troubled his house, and devoured his substance, and oppressed his child.” Then wise Penelope answered her: “Dear nurse, the gods have made thee distraught, the gods that can make foolish even the wisdom of the wise, and that stablish the simple in understanding.'], ['How did Odysseus prove his identity to Penelope?', 'Never yet have I slept so sound since the day that Odysseus went forth to see that evil Ilios, never to be named.'], ['How did Odysseus prove his identity to Penelope?', 'Then the ancient woman went up into the upper chamber laughing aloud, to tell her mistress how her dear lord was within, and her knees moved fast for joy, and her feet stumbled one over the other; and she stood above the lady’s head and spake to her, saying: “Awake, Penelope, dear child, that thou mayest see with thine own eyes that which thou desirest day by day.'], ['How did Odysseus prove his identity to Penelope?', 'Go to now, get thee down and back to the women’s chamber, for if any other of the maids of my house had come and brought me such tidings, and wakened me from sleep, straightway would I have sent her back woefully to return within the women’s chamber; but this time thine old age shall stand thee in good stead.” Then the good nurse Eurycleia answered her: “I mock thee not, dear child, but in very deed Odysseus is here, and hath come home, even as I tell thee.'], ['How did Odysseus prove his identity to Penelope?', 'But long ago Telemachus was ware of him, that he was within the house, yet in his prudence he hid the counsels of his father, that he might take vengeance on the violence of'], ['How did Odysseus prove his identity to Penelope?', 'He is that guest on whom all men wrought such dishonour in the halls.'], ['How did Odysseus prove his identity to Penelope?', 'BOOK XXIII.'], ['How did Odysseus prove his identity to Penelope?', 'Why dost thou mock me, who have a spirit full of sorrow, to speak these wild words, and rousest me out of sweet slumber, that had bound me and overshadowed mine eyelids?'], ['How did Odysseus prove his identity to Penelope?', 'They it is that have marred thy reason, though heretofore thou hadst a prudent heart.']]\n",
      "I'm not entirely confident about this answer. Would you like to rephrase or ask about another topic?\n",
      "\n",
      "I'm unsure about the answer to this question because there is no mention of how Odysseus proved his identity to Penelope in our conversation history or the provided context. The context only mentions that Odysseus \"maketh himself known to Penelope\" but does not provide details on how he did so.\n",
      "\n",
      "Confidence: 28%\n",
      "Sources: Pages 278\n"
     ]
    }
   ],
   "source": [
    "print(ask_question(\"How did Odysseus prove his identity to Penelope?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anandhu\\AppData\\Local\\Temp\\ipykernel_5520\\1170359079.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  probabilities = torch.sigmoid(torch.tensor(scores)).numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerank Pairs: [[\"What role does Athena play in Odysseus' journey?\", 'But when he was now about to enter the pleasant city, then the goddess, grey-eyed Athene, met him, in the fashion of a young maiden carrying a pitcher, and she stood over against him, and goodly Odysseus inquired of her: “My child, couldst thou not lead me to the palace of the lord Alcinous, who bears sway among this people?'], [\"What role does Athena play in Odysseus' journey?\", 'At that same hour Odysseus roused him to go to the city, and Athene shed a deep mist about Odysseus for the favour that she bare him, lest any of the Phaeacians, high of heart, should meet him and mock him in sharp speech, and ask him who he was.'], [\"What role does Athena play in Odysseus' journey?\", 'Odysseus being received at the house of the king Alcinous, the queen after supper, taking notice of his garments, gives him occasion to relate his passage thither on the raft.'], [\"What role does Athena play in Odysseus' journey?\", 'So he prayed there, the steadfast goodly Odysseus, while the two strong mules bare the princess to the town.'], [\"What role does Athena play in Odysseus' journey?\", 'But the maiden betook her to her chamber; and an aged dame from Aperaea kindled the fire for her, Eurymedusa, the handmaid of the chamber, whom the curved ships upon a time had brought from Aperaea; and men chose her as a prize for Alcinous, seeing that he bare rule over all the Phaeacians, and the people hearkened to him as to a god.'], [\"What role does Athena play in Odysseus' journey?\", 'Alcinous promises him a convoy for the morrow.'], [\"What role does Athena play in Odysseus' journey?\", 'And when she had now come to the famous palace of her father, she halted at the gateway, and round her gathered her brothers, men like to the immortals, and they loosed the mules from under the car, and carried the raiment within.'], [\"What role does Athena play in Odysseus' journey?\", 'She waited on the white-armed Nausicaa in the palace halls; she was wont to kindle the fire and prepare the supper in the inner chamber.'], [\"What role does Athena play in Odysseus' journey?\", 'Lo, I am come here, a stranger travel-worn from afar, from a distant land; wherefore of the folk who possess this city and country I know not any man.” Then the goddess, grey-eyed Athene, answered him saying: “Yea now, father and stranger, I will show thee the house that thou bidst me declare, for it lies near'], [\"What role does Athena play in Odysseus' journey?\", 'BOOK VII.']]\n",
      "I'm unsure about the answer to this question because there is no mention of Athena's role in Odysseus' journey in our conversation history or the provided context. The context only mentions that Athena, grey-eyed, answers Odysseus saying \"Yea now, father and stranger, I will show thee the house that thou bidst me declare, for it lies near\", but does not provide details on her role in his journey.\n",
      "\n",
      "Confidence: 68%\n",
      "Sources: Pages 91\n"
     ]
    }
   ],
   "source": [
    "print(ask_question(\"What role does Athena play in Odysseus' journey?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anandhu\\AppData\\Local\\Temp\\ipykernel_5520\\1170359079.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  probabilities = torch.sigmoid(torch.tensor(scores)).numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerank Pairs: [['What happens when Odysseus returns to Ithaca?', 'Athene again urges the release of Odysseus; and Hermes is sent to bid Calypso let the hero go.'], ['What happens when Odysseus returns to Ithaca?', 'They reach Pylos, and are kindly received by the aged Nestor, who has no news about Odysseus.'], ['What happens when Odysseus returns to Ithaca?', 'Zeus prophecies that after twenty days sailing, Odysseus will reach Scheria, and the hospitable'], ['What happens when Odysseus returns to Ithaca?', 'On this same day (the sixth) the wooers in Ithaca learned that Telemachus had really set out to “cruise after his father.” They sent some of their number to lie in ambush for him, in a certain strait which he was likely to pass on his return to Ithaca.'], ['What happens when Odysseus returns to Ithaca?', 'He had heard from Proteus, the Old Man of the Sea, that Odysseus was alive, and a captive on an island of the deep.'], ['What happens when Odysseus returns to Ithaca?', 'Menelaus tells how he himself came home in the eighth year after the fall of Troy.'], ['What happens when Odysseus returns to Ithaca?', 'It will later appear that he made an even longer stay at Sparta, though whether he changed his mind, or whether we have here an inadvertence of the poet’s it is hard to determine.'], ['What happens when Odysseus returns to Ithaca?', 'After sacrifice, Athene disappears.'], ['What happens when Odysseus returns to Ithaca?', 'In the evening Telemachus (leaving his ship and friends at Pylos) drives his chariot into Pherae, half way to Sparta; Peisistratus, the son of Nestor, accompanies him.'], ['What happens when Odysseus returns to Ithaca?', 'Penelope also heard of her son’s departure, but was consoled by a dream.']]\n",
      "I'm not entirely confident about this answer. Would you like to rephrase or ask about another topic?\n",
      "\n",
      "I'm unsure about the answer to this question because there is no mention of what happens when Odysseus returns to Ithaca in our conversation history or the provided context.\n",
      "\n",
      "Confidence: 56%\n",
      "Sources: Pages 16\n"
     ]
    }
   ],
   "source": [
    "print(ask_question(\"What happens when Odysseus returns to Ithaca?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[151], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mask_question\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the significance of the bow in The Odyssey?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[143], line 4\u001b[0m, in \u001b[0;36mask_question\u001b[1;34m(question, confidence_threshold)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mask_question\u001b[39m(question, confidence_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.65\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Retrieve context (Top-K Chunks)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     initial_chunks \u001b[38;5;241m=\u001b[39m ensemble_retriever\u001b[38;5;241m.\u001b[39mget_relevant_documents(question)\n\u001b[1;32m----> 4\u001b[0m     final_chunks, relevance_scores \u001b[38;5;241m=\u001b[39m \u001b[43mrerank_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Calculate confidence\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     confidence \u001b[38;5;241m=\u001b[39m calculate_confidence(relevance_scores)\n",
      "Cell \u001b[1;32mIn[136], line 20\u001b[0m, in \u001b[0;36mrerank_chunks\u001b[1;34m(query, chunks, top_k, max_context_tokens)\u001b[0m\n\u001b[0;32m     17\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(pairs, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 20\u001b[0m     scores \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     22\u001b[0m sorted_indices \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39margsort(descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m selected_chunks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:1327\u001b[0m, in \u001b[0;36mXLMRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1325\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1327\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1338\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1339\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:977\u001b[0m, in \u001b[0;36mXLMRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    975\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 977\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    989\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    990\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:632\u001b[0m, in \u001b[0;36mXLMRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    621\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    622\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    623\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    629\u001b[0m         output_attentions,\n\u001b[0;32m    630\u001b[0m     )\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 632\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    642\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:563\u001b[0m, in \u001b[0;36mXLMRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    560\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    561\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 563\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    566\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    568\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pytorch_utils.py:255\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:576\u001b[0m, in \u001b[0;36mXLMRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    575\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 576\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:487\u001b[0m, in \u001b[0;36mXLMRobertaOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 487\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    489\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Anandhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(ask_question(\"What is the significance of the bow in The Odyssey?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ask_question(\"Who are the suitors in The Odyssey?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ask_question(\"What is the Cyclops' name in The Odyssey?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ask_question(\"How does Odysseus escape from the Cyclops' cave?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ask_question(\"What happens to the men who eat the Lotus flowers in The Odyssey?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
